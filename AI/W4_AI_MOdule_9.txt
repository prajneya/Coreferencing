Welcome back friends. We are into ninth module of AI. We will be discussing about Neural Networks this time. We have looked at a few problems and the method of using state spaces in solving them. Those problems were quite structured, quite structured in the sense that we can define something called a start state, something called a ninth state in different states spaces and we can define rules which can let us move around in state space, moving from start space to the end state. In some cases it is not possible.

For example, if you want to recognize a signature the problem itself is quite fuzzy, not only the problem is fuzzy though we are able to effectively solve that problem, we will be able to recognize a signature, we will not be able to tell exactly how we have done it. So that is the first thing, no algorithm issue. You meet your friend after many years and he is doing all dimensions but you still be able to recognize it. How have you done it? it is again is something that cannot explain. There is no algorithm that you can talk about. That is the problem. Here we can change this, we can actually mimic this by changing the program itself, the way we could. How it is done?

In long back, long back even Sixties computer scientists who were working on such problems understood that these problems are different, they are not similar problems. A human being can easily do it, can deal with this incomplete fuzzy things and so and so forth, but conventional computer program could not do it, so they thought, that let us try to see how human brain does it and they worked on it and they tried to mimic the working of functioning of human brain into compute programs. What they try to do is they tried to learn the neurons, how the neurons of brains are functioning and try to mimic the very neuron in the computer domain that is called artificial neuron. And using artificial neuron they try to solve those complicated seemingly complicated problems. I use worked seemingly complicated because recognizing a signature is not a great job, you probably very low intelligent person can also do that, it does not seem to be all that complicated, looking from only the programming perspective it is pretty complicated otherwise it is not all that complicated. 

So the point is researchers started working in the direction of designing mimicking human brain by providing algorithm which can work that way, and there are many models in fact. Lot of people came out with lot of models and plenty of model that exists in literature. What we are going to talk about is preliminaries, few basic things about neural networks.

Once they do it, once they are able to mimic the human brain, they are able to make many other things. They are able to solve some problems unsolvable for years. Even today it is being so, it is able to solve lot of problems for example recently goggle provided a solution, you just look at the image and can ask for something which is otherwise just impossible, you can ask for the color of the tail of the cat or ask for the hat the tennis player is wearing and the computer program looking at the image can answer you, such programs were just impossible a few years back. This is because of lot of advancement in this particular field this new neural network.

So in this particular module we will be discussing about what neural networks are in subsequent to more modules we will be looking at one or two very, very powerful algorithms of using neural networks, okay. It is a separate discipline today. Neural network is come out of AI, is no longer part of AI, but still people talk about Neural Network when they talk about AI, Neural network has become a discipline on its own. And there are lot of problems like recognizing a face, recognizing a signature. Interestingly two of my students implemented neural networks for very different cases. One particular student implemented this to shortlist a resume [Indiscernible][0:04:01] five thousand odd resumes is coming in daily and what they need bunch of experts who could look at those resumes and filter people whom they can interview and it is a expensive process. Because experts will have to look at all those resumes. What he did, he wrote an artificial intelligent program based on neural networks and when he could do it the advantage of that program is that could shortlist those five thousand resumes into just five hundred and expert work is reduced stand fold, okay. They could concentrate more on more better resumes and it could result into fast far better short-listing process. So that is one example. The other example is one more of my student who used neural networks to find out best set of motivational strategies for employees. I will be talking about that in due course.

Basically, what we want in neural network solution is to mimic brain, human brain. This brain is very different than computer, the computer processor and now we will be discussing about the differences between these two. Looking at the differences, we will be probably have some idea what is some different in computer artificial neuron, in the brain compare with the processors and the; we will learn about the motivation for having the artificial neuron. The first difference is that the neuron is very slow, it operates in the range of the milliseconds or processors operate in the range of nanoseconds so there is no match here. Neuron is very tiny thing and it uses electrochemical reactions, the signaling process to send and receive. Here the computers use electromagnetic waves you know. Neurons memory is pretty less, it show very few things. There are totally ten is to ten neurons in the brain and total ten is to 14 interconnections that means, almost one thousand connections from a neuron to other neurons.
 
And this connections are dynamic, they change over a period of time and this connections are weighted, they contain weights and this weights basically carries knowledge that we have in our brain. Point is, there is a huge distributed interconnected dynamic structure. The structure is not only dynamic, the structure is also completely distributed; every neuron takes a decision on its own. Others do not take decisions on its behalf. There is no central authority unlike computer processors. It is completely distributed and all neuron works in parallel. At least quite fault tolerant. In the sense that, a neurons are constantly dying. Since birth neurons do not remain their steady living, they are constantly dying. But we do not lose important information there in our mind, okay. The – the beauty is that it stores information in distributed fashion and not a localized fashion like computer programs. So it will never have that data loss kind of an issue. Sometimes somebody do have but usually our brains are capable of making sure, even if the neurons die they do not lose data. Okay, so that is something which is very different from a computer program, computer system, and – and human brain. 

One more thing is that it is capable of using looking at fuzzy things and manage, we can, we are moving at a cricket match and you see a side face of friend and you recognize him. You are able to work in-- with a partial information. You have incomplete information. You have been to a place long back and you visiting again. You do not have complete information about the current status but you will soon be able to find your own way out based on the old memories little part of something that you can remember. So the idea is that we are, the brain is capable to manage fuzzy and incomplete information. The design itself is that fashion and they work in complete thing, the neuron, anywhere have any issue of synchronization unlike that however fast this processors are and number of processors can also much larger than neurons. The issue is synchronization. Scientist have not yet found a way to sync those processors in an seamless way, so people are working on it, there is no solution yet. 

Obviously, there are not algorithms here. Brian solves many problems without algorithm recognizing somebody is one example. Hearing few words of music and recollecting the entire song is the other, there are many similar things. A search is an associative way is one more, for example I say that there was a movie where a Shahrukhan, Deepika was there, that parsi actor was also there. I won’t name Boman but I just say that there was a parsi actor, and you would immediately come back it tell it was Dilwale, how did you do that, no sorry it is Happy New year. How have you done that? You did that because you could associate this information and search in associative way. We have talked about this in past. So this associative search is also something where human brain are far better compare to us, the computers. 

As I said, researchers try to mimic brain. So the first thing that they wanted to do which is to have a neuron which they call artificial neuron. Okay, so – the many researchers they did many develop many artificial neurons, we are not going to discuss all of them we are just going to talk about the most preliminary. First of all let us look at the basic brain neuron. Basis brain neuron is roughly divided into three parts, one is called Dendrites which is basically an input device, something which provides input to the cell, the nucleus of the neuron which has a long cord which is called an Axon and which again it is distributed in small tiny threads okay. So this Axon is basically an output device. So whenever an electrochemical signal arrives at neuron the dendrites will capture that, send that thing to the nucleus, the nucleus process is that and then send it across over Axon, Axon connects itself to dendrites of other neurons. And as said, such connections are plenty, on an average one thousand connections. 

So every neuron on an average is connected to one thousand of the neuron, a huge network and highly connected network, okay. So this is – the artificial neurons also is designed exactly in that fashion. So you can look at the next slide. What does this artificial neuron do? The nucleus job is done by a function called the Sigma. There are sometimes other function used, you will be looking at few other functions in due course. But this Sigma is basically summing up everybody that comes in. Now remember brain uses a stimuli information that is coming in + weight, okay. So here also we have inputs, the inputs are x1 to xn and there are weights, weights are w1 to wn, okay. And sometimes an additional value is provided. In the slide you will see that there is an additional value one which is provided w one, w two one, w three one, w n one. Now you maybe surprise what that one is. One is called layer. Many a time the neurons appear in multiple layers. So this value describes weight of which layer, okay so we will come across this once again. Some authors describe the layer at the end. Some authors describe the layer in the beginning. So it – you can write this thing as w one one, w one two, w one three and w one and so on. That is also is perfectly valid because that also is acceptable, acceptable in literature. The sigma does what, it just sums up things. So the every line has two components, xi and wi, so it just multiply xi wi xi wi x1 w1 x2 w2 x3 w3 at this point of time I am not bothered about layers, so I am just writing it that way, so it is basically sigma xi wi where i ranges from one to n. That is the first thing. And you can see the row the next one the row value. The row value is basically a function which checks for a threshold value. If the summation is beyond the threshold value a typical threshold value it outputs one otherwise it output zero. A very simple function, it outputs zero or one based on the total summation of wi xi. So wi xi is going beyond a point you output one otherwise you output zero. So that is this function. So the artificial neuron you can see that it is pretty simple design. 

The next slide talks about the same. You can see that the first line say x one, w one k, x two w two k. Now here k again is a layer which I was talking about. K also can be there in the beginning, okay. The subscripts that we are using here one to n. The subscript order is the designers choice, there is no fixed rule. Okay. Now interestingly when you start doing it this way there can be three different types of outputs. What are those types? The first thing for an input, for example I am taking a very simple case of recognizing an email. So what I do is I provide images of five different friends of mine and I want the neural this neuron to learn to classify them. So I just provide those images as inputs to my neuron and those inputs and I pick up some random weights, weights are multiplied with the inputs and the summation is generated. Okay, let us take a simpler case of having just one friend, so the output is whether is the friends image or not, so we will have only two different types of images. Images of the friend that we would like to recognize and some other images, only two things, so that simplifies our description. So when this happens, it is quite possible --  there are three possible outcomes. One outcome is that I provide image of the friend and it recognize it correctly. So for example, I assume that for correct emails the output should be one and I get one, so that means great. This person is recognized clearly. But then there are two issues. One I may provide the image of my friend but the output says he is not. Image of some other object is provided and it says it is a friend. Now here please understand there are two different problems. Both of them are misclassification. In case number one, we say that it is the summation is beyond greater than the threshold value there is a problem okay sorry greater than the threshold value is friend but we provide something else and still summation is beyond that. So that is called positive misclassification, okay. The other way around, if you provide the friends image and the summation is less than what is expected is called negative misclassification, okay. So classification can be only one, correct classification, right image friends image is friends non-friend is non-friend, great. But then if you provide friends image you should say friend and it says the other one, the other one is provide says friend these are the examples of misclassification. Okay. There is one more thing. The row which we discussed, in fact we can assume it to be part of information is again very simple what we are doing is x one w one plus x two w two x three w three and so on. We can say that this row, we call it w zero, so it is basically and we can take it on the left side of the equation, what we have is x one, w one, x two w two at the end minus w zero greater than zero then it is firing less than zero it is not firing, a output one or zero. We also have x zero assume x zero as one and w is minus w zero is minus threshold then the equation is very simplified. It is x zero w zero plus x one w one and so on and it is greater than zero then it fire us less than zero it does not. Now this is simplified version of the vary function. Remember x zero is not an input, it is a value one and w zero is minus threshold value. Okay. This function is known as activation function. The first activation function that we look at it is called Square which we have already seen. The Square function is sigma equal to zero to n xi yi. If it is greater than zero it fires, less than zero it does not. Okay. It is very simple. If you draw a graph of it, you will be able to see that it is basically square. We will just see that soon. The next one is called sigmoid. Now that is little different function. The same summation is calculated but then what you have is one upon one plus e raise to minus sum, you just calculate it that way, it is there shown on the screen that you better see that yourself, okay, so this is called sigmoid function. Physically the square function look the same as shown on the slide. The next slide talks about the shape of square function. Okay. What is called – you can see why it is called square. It is like this, okay it is basically a discrete function for all negative values it is zero, for all positive values it is plus one. In fact the vertical line that you see from zero to one is not actually indicating any input it is just shown to make sure that you it looks like square. The sigmoid function you can see is, is S shaped. When you approach minus infinity it become zero. When you approach positive infinity it approaches one. Okay. That is sigmoid function which you can see drawn on the second slide, okay. The first one is the discrete function you can see that the second one is not discrete it is continues. Some algorithms which we are going to study later requires this function to be continues. Okay, so we have sigmoid function is to be used. Square function is pretty simple. It just sum things up and just check for the threshold or zero value. It is pretty simple and can also be used in embedded devices IoT devices and many other devices which does not have the power or computational power or memory to process all these. Okay. It is also used in satellite by ISRO scientist, because again over there, there is an issue about power as well as this thing, so they innovatively used this thing. They are not using square function as it is they are doing something else. Sigmoid because it is continuous it is possible to differentiate and differentiation is very required, why it is required, sorry I cannot talk about right now, if you look at the mathematical background of some algorithms like Backpropagation you probably will be able to understand why differentiation is must. But understand that this is a very, very critical requirement for a function to be used in Backpropagation like algorithm. Back propagation algorithm is something which is used to train neural networks. We will be looking at that algorithm in the next module. 

And interesting thing is if you look at the shape of both functions, if positively classified, correctly classified value for example, for your friend the output should be greater than zero and you get zero point one. Now is it a good thing to have? It is not very good. Because if later the weights would change later this value might become negative. What we want is we want much larger value, the summation should be beyond, it should be far from zero in a way that later on if some weights are reduced will still be more, so it will still be able to recognize that image of your friend. So you need to increase summation even for correct answers. But here it is not possible in square. In sigmoid will continuously increase because if you want to get one actually it is sigmoid will output value between zero and one. And if you want to get one you will have to get the summation should be positive infinite which is very hard, almost impossible to get. Similarly, if you want to get zero you will have to get negative infinite which is also impossible, so it continuous to learn even if it has learn that thing correctly. Most of the cases what we do is we provide a cap, say that when it reaches ninety percent nine point nine we accept that thing or minus point one then you accept that thing as well. Instead of zero we accept point one instead of one, we accept point nine just because of this problem. And it is also called tolerance level. So a tolerance level is point one. What is a learning? I just said a few times a processors that this is a processor learning, what exactly is a processors? Learning is a process which for getting such a weight which can identify every input as correctly. But we do not know what those weights are. So what we do is we begin with random set of weights and somehow make sure that this weights are changes over a period of time and converse into the set which gives us the right combination right answer for every input that we provide. How it is done? In fact it is done, I talked about friend now I can talk about thing similar called recognizing faces of employees of the company. Okay, so you provide those employees twenty, thirty employees and their fifty odd images of each one of them is [indiscernible][0:21:53] into the system. And if it recognizes it is fine, if does not recognize you will have to change those weights, okay so if it does not recognize you just change continuously change those weights eventually it will converse into that set and you may ask me if you ask me what is the guarantee that you have. Let me tell you there is no theoretical guarantee of this process. But empirically it is proved that in most cases you get them. So that is it. What we do is, we just get all those images, store those images in the database, provide each image to the algorithm, we call it neural network algorithm or Backpropagation algorithm or it was some other algorithm. After every input we adjust weights. And once we provide all inputs all employees all images twenty employees, fifty image of each one of them, one thousand total images all of them are [indiscernible][0:22:46]; once they are done, weights are adjusted one epoch is said to be over. 

And you will have to do it again, second epoch, third epoch. In normal cases you require twenty to thirty thousand epochs. And sometimes you need to have more number of epochs depending on the complexities of the images that you provide. You have to continue doing it unless and until you get the final result. But remember what we need to do is what we need to do for positively misclassified and negatively misclassified. Remember the idea. In case of positively misclassified case what has happened, the summation should be less than zero but it is going above zero. You will have to reduce the summation. You cannot change size because that is the input, that is the image that is coming in, you cannot change that. You can only change the weights. And again input is of two types. Zero and one. Okay. Those bits are what zero and one. So anything multiplied by zero will be zero, xi wi k is all xi is where the xi value is zero wi does not make any contribution to the summation. So what we have to do is wherever the xi is non-zero one we have to change the value of w. We have to reduce that value because we want to reduce the amount of summation. 

Now the answer is that what we do is will reduce the weights of every case where xi is non-zero. Hopefully the summation will be less than zero now. Similarly, when you have negative misclassification it is the images of your friend and it is not saying yes, then we will have to increase the amount of summation and again anything multiplied by zero is not contributing, so will only increase. Now we are increasing, we increase the weights associated with all non-zero inputs so that is what we are going to do in learning process. We have already seen that even classified correctly classified inputs are also learning, in case of sigmoid. For square it is not, okay. The default form of square does not actually do it. The slope of an activation function also is an important thing here. Because if you see that slope okay. Now if you increase the summation the value is available on the y axis, larger the summation higher the value more the value. Look at the sigmoid function you will be able to see. You take the summation further and if the value is more so will continue increasing you will get a better value. If the slope is like this, if slope is more steep then the earlier case, little change in a way it will increase the output much further. So it learns faster. What point here is that you can decide the slope of an  activation function for – if you want to learn faster and all that. Okay, so that slope can be adjusted. Okay. Learning faster is not always advisable. So but – you may ask let us keep it like this so it will immediately bump up and learn, no. That is not a very good idea, usually it is little forty five degrees okay is most common case. But you can change that slope here and there to improve the speed and let the algorithm solve the problem and faster sometime that is needed. Okay. The other thing how many samples do I need? In fact I just said that for twenty employees each of the employee I have fifty images. Now how I decided those fifty images? There is no thumb rule for it. People have done research and people have worked on it and the rough relation is that the more complex the figure is you require more learning, okay. So more number of samples. Ideally the sample should contain everything that could there, for example, if my signature is to be recognized and I will have to actually try writing my signature twenty, thirty, forty, fifty times and hopefully it should cover all different variations I produce while I sign okay. Once that is done then we will able to recognize my signature much more correctly, okay so that is how it is done. But then there is one more important issue. The neural network should not only recognize my system my signature but should which I already presented or should not only recognize the image which is presented in the database but should also be able to recognize some other image of the same person and that is called generalization. And this is something which is there in humans as well like if you teach a student how to draw A and recognize A any size A he should be able to recognize later so that is called generalization. And we expect artificial neural network is to exhibit that characteristic. And in fact this method though we have not yet specified how it is, so we will look it later. It is possible to generalize in fact this method is good enough for us to generalize. And this process is like black box. The system works like black box. The inputs are provided, output are generated and we correct those outputs by changing these weights. We do not really know how these weights encode the knowledge, but somehow at the end is weights are able to recognize signature, finding out the who the person in the image and so on and so forth. They are able to do it. But the knowledge, we do not really know how it is being done because it is encoded in form of weights, that is why it is called the black box of reasoning. Our brain somehow, in brain also it works that way. It stores that information in some convenient form which is not known to anyone of us, okay. Many, most of the vision of the audio related information is stored that way. In fact, there is one more important type of learning which is left. So far we have said that we provide input, we say that this is the right output, the program generate a wrong output we convert that, we change it to make sure it generate the right output. Not always we learn that way. Okay, many a times we look at things and classify them. For example, we look at essay and decide that this is a good essay and this is a bad essay. We meet people and decide this is a good person and this is a bad person, this is not so nice person and so on and so forth. And how do we do that? We look at their characteristics and somehow relate people with similar characteristic together and group them together, that grouping or clustering of inputs also is very important, that also is type of learning that is called unsupervised learning, okay. Unsupervised is because you do not have any feedback, you just do it on your own, okay. There is no feedback whether it is right or wrong. In case of supervised learning, what you provide for example, given image of an employee of a company and it produces that this guy Ram is not Ram he is Shyam should be able to tell and should be able to change the weight in a way that next time the same images provided it should say it is Ram and not Shyam. That is not happening here. Okay. That is why this is called unsupervised learning which is also quite important, and many a times unsupervised learning is as a process pre-process for the supervised learning. With that will come to an end of this module. What we have seen in this module? We have introduced Neural Networks in this module, we have seen how brains work, the artificial neuron and then we have looked at how this artificial neuron actually can process data, we have seen how threshold value can also be used together with the other inputs and we have seen two activation functions square and sigmoid. And we have seen why a function like sigmoid is used in some of the algorithms that neural network is using. With that will conclude. Thank you.   
            

