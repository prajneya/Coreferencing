Hello Friends! We have already seen a few things about AI and search methods. We have seen what AI is and we have seen how to represent problems using state space and we have seen how to search. We have seen unguided search methods. We looked at some guided too methods actually hill climbing and [Inaudible: 00:38] search in earlier module. In this module we are going to see some other search methods, little better methods of searching. They are called… I'm calling them other search methods… some in fact we are going to have one more module after that which we are going to discuss some typical search methods. What's the difference here? The difference here is… is about the… the application of heuristic function nothing other than that. One of them is called simulated hill climbing for example. Simulated hill climbing is a method where… sorry simulated annealing  actually simulated annealing is… is basically a superset of hill climbing that we've seen in the earlier module which allows some random movements in… in the other direction. We will also see something called variable neighborhood descent, which is again little different, which actually is better for solution search which… which we have just seen last… in the last module. So, all in all, we are going to see how heuristic function is applied in other search methods. We’ll begin with something called Variable neighborhood descent. This is the first other search method that we are going to see. Now the earlier module we described both methods which are… they are they are quite common and they are quite general in the sense that many variants of them exist in… in… in the real world. Variable neighborhood descent is a little different type of an algorithm. For example, let us again take that solutions search method of missionary cannibal problem. 

Suppose if I'm… I’m I have a bunch of states say twenty different states and I try to see if this sequence is good enough for me to get a solution. If it is not, what should I do? I may say that I'll only change two of the states. So out of ten if I… if I can only change two. And again, I will say that only two consecutive states. I'll have only ten different children. If I say three, I have many more. If I say any two I'll have again much more. So point is I can actually decide number of children that I'm going to have, if I'm exploring solution space search. 

If I'm… I'm using a small number of nodes as an average number of nodes that you produce, number of children that you produce it’s called Sparse neighborhood. And if I have large number of nodes, it's called Dense neighborhood. The function which gets me that is called Neighborhood function. Now, let me repeat what I'm talking about. Given a solution state, I can actually generate multiple solution states from it. Depending on the condition that I use for choosing them, I can have less number of nodes or more number of nodes depending on the function that I choose. It is possible to choose a function which produces less number of nodes which is called Sparse neighborhood function or I can have more number of nodes which is called Dense neighborhood function. Okay. Why sparse neighborhood is to be chosen? Please understand, and this is not only in the case of missionary cannibal. In fact sparse and dense neighborhood can also be applied in… in conventional constructive search. For example, let is again take chess. There's something called plausible move generator. A plausible move generator is a program what… what does it do? It just looks from the… all possible states… states that you can generate will only choose some of them it will not choose all will choose only few and plausible better moves compared to others. Okay, something which can which can lead to solution, consider… okay. It's very similar to human chess player. For example, if you're playing chess, you won’t to look at all possible moves, you only look at some which are good, which are…which are better compared to others. 

Okay, and usually when you write a game playing program, play around with game playing program very often that function is known as MoveGen move… Move generator. Okay. And I've already talked about sparse neighborhood and dense neighborhood and there is something called variable neighborhood. Now that’s little surprising what variable neighborhood could be? But simple in the initial chess movement. For example, you’re moving across you won't look at all moves okay. But when the situation becomes critical, what will you do? You will try to actually explore more number of nodes. For example, when your king is under attack, well and then there is a fork, possibility of fork. You try to explore all possible states and you become more conscious about… In a way you're applying dense neighborhood function when the situation becomes critical. Also in the case of endgame you do that. 

So, that is called Variable neighborhood. In the beginning, usually you have a sparse neighborhood and then you go in depth you have a dense neighborhood. It is quite possible that you apply the same MoveGen function and what will it do? It will actually for example, generate hundred possible moves and out of that it explores only first twenty.  Okay. In next case we’ll explore all hundred and explore only first five, okay. So in case of exploring only first five is a sparse neighborhood case, exploring twenty is little more dense, exploring ninety is really dense. Okay, let us look at this figure. A chess board, you can have this two states. Now it is actually possible to have around twenty different states. Moving upon one step further or two step further and you have eight points. So total sixteen actually. So out of that sixteen I'm just choosing two and not more. So, I'm… I'm not… or for example, there are twenty moves and I’ll just choose two. This is the other example. Six two out of sixteen but there is a possibility that there is twenty and then you choose two. So what's the saving? Saving is not these moves, but then the entire [Indiscernible: 6:57] which are generated by those moves. 

Okay, for example missionary cannibal we've already… I've already talked about that and this slide actually is talking about that again. Take a set of random sequences as nodes in the state space, the solution space. Well when we reach to an unreachable or unaccepted… Now there are two things you have for example, there are states one two three four  and when you… you go to a state you will find the state, remember missionary cannibal there are some states which are not acceptable. For example when number of missionaries are less than number of cannibals you cannot accept that state. So, if you reach to a state which is not acceptable or when you reach to a move which is not acceptable because it is impossible using the move to move from one… this state to this state. 

So, in either case, this particular sequence is not useful. So what you can do is you can apply variable, the neighborhood descent function. If you use sparse what you can do is you can just replace it by the remaining. Suppose example there are ten moves and you reach to seventh move which is not accepted, so seven eight nine ten four moves you can take all combinations of that and try. So that is one way and then it is quite possible that you take the entire sequence again. It is huge. So first example is comparatively sparse than the second one, okay. Why sparse neighborhood to be used? In fact it is not good obviously because you're not exploring  all nodes. It is possible that you miss an optimum par you miss a solution altogether. So why? For a simple reason that would like to avoid unnecessary calculation of moves. If you avoid eighteen moves for example, the entire eighteen sub trees are pruned from the search tree. That's the power. Just take the case of branching factor twenty three plies only three levels in game the level of tree is called plie. It is eighteen into twenty into twenty for twenty being a branch factor, look at the value I can't read that. And if it is four plies one four four zero zero zero in case of twenty, seven seven one seven five zero in case of thirty five for five plies you can see is going way beyond. 

Okay, so that's the… that's the saving that you will have okay. Obviously you're paying the price but if your heuristic function is good enough and you're choosing the best moves, then you're going to save a lot of memory as well as computing power and you will be able to complete the… this thing in time. The problems could be if you're using sparse neighborhood we might be ignoring some important moves. We might end up being on top of local minima. We may end up on a dead end because you explore only two and there are no… both of them are dead ends, okay. 

So better thing is choose the function on demand. For example, you begin with a sparse neighborhood when you stuck up when you have a problem when you have local maxima try dense neighborhood and  try an alternate route, okay? Why… when you do… when you go for dense neighborhood okay why… We have already seen why sparse neighborhood. We like to explore a search space much faster and real time. So we’ll… we’ll use sparse neighborhood. But when you will choose when you switch over from the… the sparse neighborhood to dense neighborhood. One example I've already given later part of the game when it's pretty crucial. For example, our… our… our critical cases like king is under attack, threat of fork which is basically mimicking how human player works okay, which obviously will take more time, explore more number of nodes and also look at more number of moves and also deeper.

Okay, so when you use this variable neighborhood descent and why it is called descent please understand. Because the… the heuristic function value in this case is decreasing and not increasing. But technically there's no difference. For example, if you use a heuristic function called H which for a better move gives a higher value of heuristic function. What if there is a function called H dash which just negates that value? What you get is a lower value. Okay, so technically there is no difference. It is only the magnitude the sign which actually makes the difference but otherwise technically there's no difference. In case of heuristic, value being more in… in the later cases it’s called Hill climbing. In case of being decreased for a good move it is called valley descending. But okay it’s our choice of words basically otherwise it’s same.

Okay, so the neighborhood I've already told you that in most cases use sparse, you use sparse neighborhood in the beginning and then dense at later stages. The process is basically descending and not ascending like hill climbing. And the function obviously is called heuristic function rather than objective function. You may… you ask why the function is called objective rather than heuristic? Okay, it's just the test nothing else. There’s no technical reason okay. The people who worked on it call it objective function. Okay, so that was variable neighborhood descent. How good is that method? That method is pretty good if you're able to use your heuristics better, okay. You can even control that thing that… that's why that method is better. But you can see I've already told you that this method is actually an extension to our [Indiscernible: 12:21] space search or hill climbing. Okay you will always add variable descent to either of them and can get a better result. Okay, the other method which we are going to see is called Beam search. Now what beam search is? Beam search is about exploring multiple paths in parallel. Now you may be surprised why you are doing that. 

Okay for example, I'm… I'm using a depth versus. I'm exploring one branch and I may also be exploring one more branch. Now why is better? That is better because the current architecture has multi core is called multi core architecture where you have computers with multiple CPUs in real sense. So it's good that you explore multiple nodes. Not only that this is also useful for exploring in real time. I'll talk about that soon what I mean, okay. You may do that, for example, you may just pick up top five, okay, nodes in chess kind of okay and pick up the best at the end. Okay So that is fine. In… in case where backtracking is impossible. For example, chess. If you have taken a path, you can't come back. This is the best method because you were exploring everything in parallels. You choose. If there is a dead end you can always try the other path. But then in rather chess it’s not all that good an example. But there is one more example. But anyway, well, I'll talk about that example soon. It provides fault tolerance. As I said, when… if you stack up somewhere, the alternate path is going to be useful and the example that I'm going to talk about is called Speech translation problem. Assume an assembly of people from multiple countries and the speaker is speaking in one typical language and there is a program called translator which what that program does is just listen to what the person is speaking and try to translate that into the language, okay. For example, somebody speaking in Hindi and the listener his friend… so he the translator is converting that Hindi sentence into a French sentence. Now, it is quite possible when… when the speaker begins with a phrase or two, it is possible that the meaning of statement is possibly in three different directions. So what a translator program will do is explore all these three branches together. Now, later on when the speaker uttered something else, which… which prohibits one of the branches, other two branches can still be explored. Now it is impossible for the speech translation program to go back and try other because it has to be continuously trans… translating everything that the person is speaking. As soon as the person completes the sentence, it is to be translated and… and delivered to the participant. You don't have time. So if… if all possible cases are explored together, if one of… if the later, he uttered some word which prohibits one of the translations, the other translations can still be tried. That's the power and that's the usefulness of beam search because here you don't have time to go back. Similar example is some surveillance case, okay. You surveilling something, okay. Then something is happening and you will have to take an action based on it. 

Now, when something is happening, if you're exploring all possible options, you will be able to take an action when something malicious is found to be happening. So if you're trying all possible alternatives, you can make the decision. You can backtrack and then redo it. It is impossible because everything is happening in real time. You have no time to go back so it's better if you explore everything together and in such a case, beams search is one of the best solutions. 

Here I have shown one example of again I’ll pick up our own eight five three gallon water jug problem and I've shown how two paths are explored in parallel. So in fact, this not a very good example of two node beam search but you can assume that these two branches which are being explored are two alternatives for… for a speech translation. Every node represents some state some typical word uttered by speaker and you're trying to like translate that statement. So that is possible. Okay, one more type of search called Tabu search. Now that is also simple but quite effective in some cases. For example, you’re doing hill climbing and you stuck there. Now if you are using pure Hill Climbing, you can't move forward. It is impossible. So, what is the solution? The solution is that you try something else. You may try one more function. In fact, you are trying multiple functions and when the one function is not yielding any result you start applying the other function. So, you use one heuristic function for example, you use one heuristic function which uses  value four for parent and three and two for children. 

Now, here you are in… enter into local maxima. You can't move forward. So, what you do is you apply some other function which gives parent say five, but then two children seven and nine then you can pick up nine. So, that is one way of doing it. There are other ways of doing it. Maybe one more way of doing it is to pick up something which is not used before, okay, some rare kind of a step. If you… if you go back and look at your… our exploration of eight five three gallon water jug problem or if  you explore this three four gallon water jug problem and so on so forth, you will find that in most cases the rare number of nodes are… are more likely to yield to solution, yield solution. So, it is possible that you choose them… you use something else to choose. Instead of using the same heuristic function, you use some other route.


 Okay, one more example… one more example is replace a move not changed for long. Something which has not changed for long, change it and see if you can reach to some solution, okay. I've already told you there’s the other one which is quite powerful is called simulated annealing is an excellent example of how one can innovate from some other domain. This basically is derived from an area which is quite unrelated to artificial intelligence called metallurgy. It is kind of, it’s one type of engineering, one engineering discipline actually. What does that do?  Physical annealing process allows the metal to cool down and reach to a stable state and it is found that the… the process of cooling down has a serious impact on the… the… the stableness of the result and it allows some randomness in the movement. What does it mean? It allows a bad move with some probability. Now, this is an important advantage of escaping from local minima. Here unlike hill climbing, when we allow a bad state. From four you can go to three also or two also or one also, but with some probability. In a way that in the initial run you… you allow that okay some… some move to a bad state from a current state is allowed. And later stage you don't. So that… that adds randomness to movement in the initial run in a way that you escape from local minima and it is proved to be quite useful. 

Okay. A process called random walk is… is about it’s very similar to our general interest. You just pick up a solution and see if it is so and in hill climbing you actually pick up the better state. Okay. In here the simulated annealing the random moves are allowed in the beginning and almost disallow them in the final one. So, it is basically hill climbing but with little randomness. So you escape from local minima, let us try to understand the two important principles of simulated annealing. First, a bad move is allowed with some probability, okay. And that probably changes over a period of time. That probably does not remain the same. In the initial run bad moves are allowed almost with equal probability as of the good move but later on it is almost zero. 

Not only that, it's kind of keeping the best so far node as well. So if you're going down, down, down, and you… it is quite possible you allow a bad move and move to a little worse state but you still have remembered the good state, so you can always go back to that state. So, because we allow the bad state you need to have this best so far. So, these are two important principles of simulated annealing. 

Okay let us try to understand the process which is derived from… In metallurgy when the new alloys are made, they are melted at high temperature and they have  to cool down in a controlled manner. And to maximize the crystal size of solid… this thing there is something called the annealing schedule which is followed. The annealing schedule is basically how you're going to cool the alloy down. If you do it fast, you will do it in short time but unlike the slow that thing you will… what you get is a high energy brittle structure which you don't like. Slow if you continue, if you do that process in a slow manner, the problem is that it takes a lot of time. And that obviously is expensive. So, what you have to do is to reduce the temperature as fast as possible without affecting the stableness of the result, okay, and that actually there is no formula. So what people do it do it empirically and find… find out the same process actually is used in simulated annealing. How? Look at this. Pick up the start node and make it current, if it is a goal state quit. Now you can see that what we are talking about is almost hill climbing otherwise best so far is current node. 

Now, there is something different. Pick up the temperature value T from the annealing schedule. Now what is this? In in case of annealing schedule it says that if the temperature is this you will have to do this. If temperature is this, you will have to do this. But in simulated annealing there is no temperature. We are talking about missionary cannibal problem and something. So this temperature is actually an artificial value that we take. Basically temperature describes or determines the amount of bad moves to be allowed, okay. So temperature value is reduced. When the temperature value is less, less probability of moving to a bad move. So, basically T is related to that. How it is related let us see. 

Apply next rule if no operator is left return best so far. So that is if there is very similar to hill climbing if you reach to a place where it is impossible to apply any rule a dead end, provide best so far. Now find out objective function value. Now here again unlike the other case, where we call heuristic function this is called objective function. Okay what's the difference? No difference okay, there's no difference just named as objective function. If this value is better, make this a current state and best so far is new state. Same as hill climbing, nothing different. Okay, but then there is a difference. Otherwise, in hill climbing we disallow. Here we allow with a probability P. How we do that is explained here. The first one generate a random number R between zero and one, okay. This is our probability. This is what we are going to use in probability. Now, we decide the objective function value of the resultant state after applying this rule what we get is a worse state than this, okay. So, we have to calculate the difference. So, we find out objective function value of a new state minus the objective function value of the current state. So, that is a difference, okay. And calculate the value P which is probability one plus E raised to minus delta E upon T. T something that we have already read. T is something that we use to control this probability. Delta is the difference between the objective function value of the current state and the new state, okay. Now how come we have this state? Because it's already there in the original one it is copied from that.

And why it is there is?… There are some… it’s called Maxwell's equation and why it is designed? There are some description about that as well which is not part of our this thing. So, we will not be exploring that here. But anyway this is the equation. We’ll calculate probability is one plus E raised to delta E. Delta E is a difference between the previous and the current state or the current state and the next state that we are trying to explore. Okay. Now this is the value P and if our R which is which we have randomly picked up is less than P, then we allow. 

Now this is basically moving to that state with the probability P because if you use a good heuristic function, good random number generator between zero and one it is equally probable each number is equally probable. So, if you choose say value P is point three so the probability of number generated less than point three is obviously point three. So, with that probability we are jumping to that node otherwise not.Okay. Okay, so there's one more thing as I said people from different domain use different words. So those who use word objective function use word operator rather than rules. And as I said objective function is the same thing as heuristic. T is temperature but not real it’s basically our remember it is in case of the annealing process, the physical annealing process temperature reduction is determined based on the empirical values and their results their… their experiments here also T we’ll decide T suppose for example, we are solving a problem and then we use one annealing schedule with T  value is  reducing with one and then we'll use with one typical way of reduction we’ll use some other way and so on and we'll find out which works better for that problem and we'll use that okay. So that is T value is decided by designer’s intuition and judgment and it is done empirically. It is done using some practical implementation and comparing with the results and you use the best one which suits you, suits that problem actually. 

So delta is decrement in the value of the objective function when you're talking about a new state you see how the decrement it happens remember why it is the decrement because you are picking up a worst state. P is the probability. What’s the annealing schedule? The   annealing schedule determines the amount of worst moves to be allowed okay. Remember the higher value of T will let you move to a worse state with more probability than less. Okay now what you do is basically you store this T value in three column table. First thing is initial value of temperature, when to reduce it and how much to reduce Okay, for example, initial value is twenty. When to reduce? Reduce after ten iterations. How much to reduce? Two so twenty will happen for first five iterations then reduce by two so, so on so that… that is what it stores. And what you have to do is to look at all answers and pick up the best schedule okay.  Which gives you the answer which gives you the answer in the fastest possible way and so on so forth so that’s about simulated annealing. Okay in this module we have seen some other heuristic search methods which are actually more used in recent times. One is variable descent neighborhood method where you use different neighborhood functions which let you determine number of children for a given state. Sparse neighborhood gives you less number of nodes. Dense neighborhood gives you more number of nodes. In the initial run, you may use sparse neighborhood to grow faster. Later cases you should use dense neighborhood to probe deeper and you may use simulated annealing as a method. Simulated annealing is a method where you allow the worst move from the… the current move unlike hill climbing with a typical amount of probability that is derived from physical process of annealing and uses almost similar method for choosing how much with how much how much how much probability  do you allow the incorrect move sorry not  incorrect it is less this thing a worse move from a  current… current move and so on. So simulated   annealing  is a better method compared to hill climbing but anyway it is also a superset of hill climbing why because if you use hill climbing with T value as zero, you only allow moves which are better which is hill climbing okay if you change that value, you allow some randomness. Usually the randomness allowed is more in the initial part of the search process and less in the lesser part of the search process. With that we end with this module. Thank you. 


