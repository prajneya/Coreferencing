Okay friends welcome back. We are on fifteenth module okay. In this particular module we’ll be looking at three additional search algorithms. One is called iterative deepening A star. You've already seen iterative deepening. Now you're going to see how A star and iterative deepening matches here. We’ll have re… there is something called RBF or recursive best first search and then we'll have an introduction to agents. What they are? Let us see you have already seen A star is a very popular algorithm and because A star is very popular, a lot of others try to adapt to the characteristics or the good things of A star and learn better and become better. So iterative deepening we have already seen. The iterative deepening you start with root node explore the first level again start from root level, explore two levels again start root three levels seemingly wasteful but found useful. As I said then we discussed about iterative deepening that such a thing if we provide heuristic value with it becomes better, okay. So, IDA star is basically that. We’ll be looking at IDA star first and then RBF okay which is recursive best first search. Both of them are memory bound they are known as memory bound searches. The memory bound… why they are called memory bound? Because they…they expect memory to be limited and are designed purposefully in a way that amount of memory that is required is always minimal. What we do is for RBF the conventional depth first search. If you remember it remembers unexplored node list of unexplored node list of explored nodes, why it keeps list of explored nodes? Because if an explored node is regenerated it’ll find that thing out. Why it keeps unexplored nodes? Because it’ll pick up the best out of that and pick up next okay, so that it keeps. 

But in case where such list are impossible to be kept due to shortage of memory RBF like solution may be used because which doesn't keep the explored list and only the best unexplored node is picked up. The advantage of this is that you… you don't need to require amount of memory that you need and obviously the cost is that you may explore an explored node yet again. Okay, so that's the problem. AI is driven by… lot of programs which are more or less automated, can interact with real world. For example, a program which books tickets, the program which arranges our tours and all that, on its own based on our preferences. 

So when you do things like that the program acts on our behalf. When a program acts on our behalf, it is called an agent. In fact, if you look at intrusion detection systems, the… there are a lot of botnets coming up. Those botnets are basically software, a small software which goes to some other computer and acts on the attacker’s behalf. Okay, so they are also examples of agents and we'll be talking about, that's the third thing that we are going look at in this module.  We’ll introduce IDA star and RBF star in short first and then we'll move to agents. They were… both IDA star and RBF were proposed by a guy called Richard Korf and he was working on as a professor working on space complexity of algorithms and he was interested in improving the cost of the solution path calculation. He was interested in reducing the amount of memory to be used. He assumed that the amount of computing power needed maybe more, but the memory should be less and he has devised this two algorithms and there are three things that he wanted to optimize precisely.

First the amount of memory that is to be used. So the cost of finding the solution itself and the path to the solution itself. And he was working on computational biology problem where you… the search space is like grid structure. So number of combinations and permutations become very, very large after a while. And when that happens, it becomes very hard for the algorithm to proceed further. The amount of memory that you need to store explored nodes the memory… amount of memory that you need to store unexplored nodes goes quite high and because of that, that leads into issue… trouble. So, the IDA star how it works. Both IDF star and RBF both… you has a linear space… space search strategy. That means the… the amount of search space at any given point of time increases linearly rather than exponentially because in case of grid it… it increases exponentially. The number of paths that you re… require to explore at every point of time is exponential in… in normal case. In this case it is linear. So that is the biggest achievement of IDA star. 

Obviously, it… it adds to the cost of… in fact, the same nodes are explored again and again because you don't remember that this node is already explored, is going to add to that time. So that's a disadvantage of this algorithm. And in one sense, this IDA star is more near to depth first this limited search that we have seen in probably the fourth module. Now the idea is very simple. Remember DFDL okay. What does it do? It explores… it assumes the length to be some value, okay. Depth maximum three or four or five now you explore that if you reach there you limited okay. Go back act like you’ve reached to a dead end and you go ahead and explore the search space for that depth only. Now in DF depth first depth limited usually you start with length equal to one then two then three or you start with three and six and so on. Here instead of that it's quite interesting move is that you don't do that. You just use a G plus H dash take a specific value of G plus H dash. For example, you just say that three G plus H dash to be three. So, what do you do use a start exploring nodes and whenever G plus H dash value goes beyond three you just stop and take a previous node and take the other branch and so on, so forth. So IDA star is very similar to DFDL. Okay, the only point is you just use G… G plus F. Now why G plus F is better? Because you know that if you traverse in the wrong direction the G and H… H dash value will go high. So when the G and H value go high the wrong direction, you won't move further much but the right direction because G plus H remember the case that we have seen in the previous module, G plus H was consistently to be six almost till the end. So those things were the… those nodes which are nearer to the solution they are explored more. That region will be explored more the regions which are in the other directions are not explored that much. That is the advantage of using this method and if you don't get what will happen? In… in most cases where H dash underestimates this you probably will not get for the first time because you've estimated to be three but is actually six. So, you won’t. So but then once you do not get it, you can increment. You can pick up the best node so far and use that thing as the measure and again try. Okay so you may continue doing that. Eventually you will be able to reach there. So the example is shown in… in the figure. The white node is the starting node and the red node is the ending node and you can see the green nodes which are inside the G Plus H. 

So you can see that more number of nodes are explored in the right direction and less number of nodes are explored in the other direction. In the second iteration, you can see that more search space is explored. But you can see that search space near to the solution is explored more. That is the power of IDA star. So, you can see that more number of green nodes towards red node in both cases, more in case two than case one and nodes in the other than right direction get their H dash H value higher in a much faster way. So you don't need to explore them more. And you can see that doesn't demand the amount of space A star needs. Why this is preferred in some cases? It uses optimized storage but costly in terms of time and the value G plus H value that you use is called cut off value. And that initial cut off value is based on root’s estimate the H dash value. If H dash is optimal in this case, we’ll get an optimal solution admissible, then we get an optimal solution like A star as here as well. We don't get a solution. We’ll just increment as I said that we just pick up the best so far node and set that value as the cutoff value and we'll start working on it again.

We might need to do it in multiple iterations, but if it is H dash is admissible be sure that we’ll get it. We get an optimal solution and the algorithm is also shown on the next slide. And we call that thing limit okay. Limit is the cutoff value. So that limit equal to H dash root node. Now you assume that from root node [Indiscernible: 8:39] three hops away you will get a solution then that value is your cut off value, the limit and the current node is root node and you start traversing like depth first search as soon as the G plus H dash value goes beyond three you just stop, you go back and take the next branch and continue doing that. If we get a solution great. If it doesn’t get a solution there has to be a best so far node. Now best so far node says that the goal is four hops away. So then you increment that to four and again do it or five then again do it and so on so forth. So that's how you do it. If G plus H as I said G plus H is greater than limit, obviously we’ll stop. Otherwise, if there is no node, find out, you change the limit value, otherwise add this node to the path okay. If node is better best node is this node. If this node is not better, the older best node will continue is like finding on the maximum number in a given [Indiscernible: 9:28] okay. So that is how it is done. This is basically a DFS with heuristics. The two different situations in this case - G plus H dash going above limit, and you calculated everything for all branches. When G plus H dash exceeds, you assume that there is a deadlock, dead end and you won't move further. 

When all nodes are explored, you just increase your limit. But remember when you increase that does not mean that you will start from where you left. You start all over again. Now that is important. You just forget everything and that is again a disadvantage of using IDA star. It does not know otherwise it has to remember the complete branches of all the complete tree that we have explored and we don't have the amount of memory we’ll… we’ll not we do not want to use that… that amount of memory in this case. So that's why it will have to start again all over again from the same place.

 Okay it reduces obviously storage requirement but that increases time because of this and it… though it… it is moving in the right direction but there is no real sense of direction. It doesn’t really know that goal node is in that direction. It just moving just jumping in all directions and based on our G plus H dash value without really having the idea about the direction. But nodes are very well connected it takes hell lot of time because it has to again look for all paths though [Indiscernible: 10:40] will have to do it again and again because it won't remember explored nodes. So that will take a lot of time and it is better for less connected graphs. If it is well connected, it will take a lot of time and that… that is not very good. Unlike the earlier case, the optimal, the admissible function does not mean that this is the best function The next algorithm is recursive best first search. It’s like best first that we have already seen but with one difference. In earlier case the best first what we do is we only store the complete set of explored nodes with set of unexplored nodes. Why we store the list of explored nodes? Because if we generate a new node which is an explored node, you don't need to worry about that further. It’s kind of a cycle okay so we will have to stop. Why we skip unexplored nodes? Because we'll have to find out the best out of it and then we'll have to choose one the best one from it. So that's why we did that. In this case the recursive the best first search we don't do that. What we do is we only pick the best… keep the best unexplored node and will only explore that and when you explore actually whenever I'm exploring my children, and I find that no children is better than the parent. When there is no children better than the parent what you do is you pick up the best so far. When you pick up the best so far the second best will be the best child, whatever the best child that you have because you don't have any other node. So that is pick up as second best and you explore this. When you explore this you find that these children are much higher value than this and you pick up this and start. 

And at this point of time the best child of this becomes the second best. So you only keep two values best and the second best and whenever you move from one value to the other, the best value becomes second best and whatever value move… you move to becomes the best. Okay so that's how it proceeds further. Like the other case IDA star case it’s like normal depth first search okay. The best node is explored but second best is preserved. It won’t preserve all other nodes only second best and if the best node is explored and all children are found to be worse, the search process backtracks to the second best okay so on so forth. I have already discussed that. Now with that we’ll we have studied two things - IDA star and recursive best first. Both of them are memory bound searches, both of them are good searches for space constrained problems. So, whenever there is an issue with space, either of the algorithm can be used. But otherwise they are not good algorithms for cases where the time is paramount in most cases it is so.

So, in that case some other algorithm is used. In fact, there are many alternates provided by many others, instead of just keeping one node you just keep the nodes which you can fit in the available memory is one solution there are a lot of similar solutions provided. But anyway this both the algorithms give us a very good idea about what people can do when there is a space constraint. So, in fact, it is quite possible that there are other algorithms and there are other algorithms which are generated from some other cases which uses the characteristics of these two algorithm. This two always are not used in serious practice, but some other algorithms are inherited from this algorithm which are actually used in practice. The next thing that we are going to look at in this particular module is called agents. 

Now what are agents? Agents are software or hardware routines or programs or devices or whatever which works on user’s behalf. A ticket booking agent for example is a manual agent. So what you do is you said I want to move from Ahmedabad to Mumbai on this date and I would prefer a second AC so get me a ticket. I will do that. It’ll look for trains, look for timings and all that look at my preferences I probably have provided my preference that I  prefer a window seat, I'll prefer if I'm traveling by day I require a chair car night, I’ll require three AC  and so on so forth. So whatever the preferences are, it’ll book tickets accordingly. So that is the job of an agent. You provide your preferences, you provide what you want to do and agent does that thing for you. So agents are the programs which work on user behalf. You probably will be… be thinking why I am… I have started talking about… suddenly jump to an agent. Okay, we so far has talked about AI and programs which are difficult and state spaces, neural networks and many other things. And we have not yet talked about agents now and we're just jumping on to it. What's the point? Why we're talking about agents? 

Anyway, when we talk about a search algorithm or for example, chess playing program or a self-driving car or whatever. Eventually the solution has to exist in real world, has to in… connect with the real world, communicate with the real world, send something, receive something from the real world and actually be a part of real world. And if you want it to work that way, the concept of agent comes handy. Because agent is basically something which works on user’s behalf or the programmer’s behalf or the designer’s behalf and do exactly what they say and interacting with the rest of the world, doing things with the rest of the world requires it to solve the problems like we are doing so far. Okay, so that's basically the same thing which we are talking about. The only thing is that we have become more explicit now. We are talking the real world case. So far we just say that chess playing algorithm is doing this. But we have not actually said how chess playing programs will work, how self driving cars will work, how a work to find out shortest route between two places. The agent railway ticket booking agent or real estate agent, even intrusion detection agent, they… they work the same. They work on user's behalf. What they do is they interact with the world. 

For example, intrusion detection system interact, is the world of intrusion detection system is the computer address space where it runs, interacts with other processes, figuring out whether a process is malicious or not and take the decision. It takes something from the world, it gives something back, and so on and so forth. So it's basically an extension to the discussion that we already have. The search spaces, the method to find solutions and all that are really implemented using agents. A Trip Advisor is another example. A TripAdvisor is going to advise you about a trip. For example you say that I want to plan a trip to Manali with my family. And the trip analyzer looking at the preferences will look at the possibilities of booking the hotel booking, the dates and all that, even my budget and other information and will do all that for me with… This is basically a software agent. Compared to that a self-driving car which let me travel from one place to the other is not only a software is also a hardware. So is hardware plus software. 

So, agents maybe hardware agent maybe software agent, maybe hardware plus software. So, these are the examples of agents and they are using whatever we are discussing so far is actually being used by agents. So, they are explicit versions of what we are talking. Another example is a parking sensor node which looks at the car being parked and decide whether the car is being parked properly or not and start beeping when it is not so and so forth. There are a lot of IoT devices appearing in this world right now okay - house monitoring system and smart agricultural system and intru… the implantable medical devices which looks at the body temperature and the blood sugar level and blood pressure and all that. So these are all examples of sensors which are taking some inputs, generating some outputs and also processing using AI. So, we… our discussion actually is confined to agents which thinks intelligently, which has to take decision based on things that we have talked about. Obviously, people call the… the agent with hardware plus software as physical agents and the agents only containing software are called software agents. 

The… and I've told you, the world of agent is different. For example, intrusion detection system the world of an agent is the operating system in which it runs or the network in which it runs and the processes which it interacts with and so on so forth. Let's call it the environment of the agent. A self-driving car. The environment is not itself but the roads, the other vehicles people who are using that car and so on so forth. The other pedestrians, [Indiscernible: 18:59] other vehicles everybody, everybody who is there on the road and comes across when using this car and so on and so forth. Point is environment is the world for the agent and most cases the agent world is confined to a small area which… and that is called environment. So whenever agent send something to the world, it uses something called actuator. When agent receives information from the world, it uses something called sensor. So agent has two components - Sensors which takes input from the world and actuator which sends signals to the world or does something in the world or the environment. the processing done in the agent itself. 

So, agent interacts with the environment using the sensors. So, agent does many things with the input for example, it takes input from the real world, process them, store them reason with them, generate output. Agent environment is… is something where the agent works and not all cases agent… in most cases agents world is confined to a very small subset of the real world. How agent works? For example, an intrusion detection agent look at some input patterns observed. It looks at what is coming up and decide whether it's an attack or not. If it is an attack, it sends the signal somewhere. If it's not an attack it let go. So, the information that it gets is called percept. For example, there was a very old attack called land attack where the… the sending IP address is same as the receiving IP address. So, if… what does the… the agent do in this case? 

It looks at the current percept or the current packet that it is looking at, current observation and it finds that the source address is same as the destination address. It decides that this is an attack and it drops it. If it is not, it’ll let it go. Now, it does not lead to look at other percepts. It only looks at the current percept and done. Now there is a complicated agent, little more complicated agent which looks at whether the user… there’s one more indication of intrusion that there are a lot of failed login attempts, normally hard you fail to log in once in a while in five, six, ten days you probably fail to log in once. On one fine day if the system find you failing to log fifty times in an hour now that is a serious thing. Now that is something very different and I want to capture such things. So, I start running an agent. So, that agent does what?  It… Can it only look at current percept and decide? It cannot it has to look at percepts of last one hour and try to figure out how many failed login attempts happen. When it does that, it’ll see that it crossing value fifty it let me know. Okay you can decide that value it is fifty or ten or five whatever. But here the difference is that it has to look at not one current percept but previous percepts as well as well and that is called percept sequence. 

So… so agent can take decision based on current percept which is simpler agent. Agent may take decision based on the percept sequence. Okay and conventional old intrusion detection systems you… were like virus scanners. You just look at the packet and it finds that the… the packet contains something which raises the suspension. It says that suspiciousness and then it just pick up that thing and drop it.

Okay so just looking at the current percept it decides. If it is not found that let it go. But then as I said there's a simpler methods. The signature recognition is… is always based on current percept but then there is a state full kind of a situation where it looks at multiple states, knows what has happened before and all that. So it remembers all that percept sequence and take a decision is a complex case and sometimes you… you learning how a user is behaving and make sure that the… the user’s behavior remain consistent. If there is inconsistency you just report. For example, as a normal user I’ll hardly access system files on one fine day I just start downloading all system files and that is something suspicious. So, that kind of thing also can be looked at, but for that I need to find out the normal behavior  so for which I’ll have to read lot of percept sequence probably a day or some… sometimes even a month to figure out what the behavior of that user. 

So for that this percept sequence makes a lot of sense. There's one more word rationality is attached to agents working. Now, how do you know that the agent is working well or doing a right job? One answer probably is it’ll decide based on the output. If the output is good, the agent is doing a real job. If the output is not good, the agent is not doing a real job… a good job, but then it is not very good. For example, if I have designed a chess playing robot. Now chess playing robot is playing something and take a silliest move but the opponent tops the program by taking a sillier move and lost. Do consider that that move was good? Or the other way around - If the… the move the chess player program has taken is really good but the opponent is very, very good and eventually could defeat so will that… will the agent consider that the move is not very good? No! in either of the cases it's wrong. 

There are many other cases depicted in the handout as well as the PPT. One is self-driving car if you're traveling in the right lane and somebody comes from the wrong lane and just meet with a… you meet with an accident that doesn’t mean or crushes you doesn't mean that you have taken a wrong decision by riding in the right lane. So that kind of situation. So you don't need to take… you don't need to measure agent’s performance looking at the output, but a better idea is to look at the objectives. And as long as the agent’s working is based on the objectives for example, user convenience. So self-driving car, if the user is convenient, it reaches to the destination the time and feels comfortable not only the user, the person who's sitting in the car, but others the… the fellow travelers also find that it is… it is fairly doing well then these are the objectives that the agent has to meet. So, when the agent is meeting the objectives, we consider the agent is doing things in a rational way.

Sometimes this is called credit assignment problem. Now, if you have done a good job, how will you assign that? Which move you think is responsible for this and there are many factors which affect outcomes which I’ve discussed. We cannot decide the rationality of the decision based on the output. We only decide whether it is the agent has followed our objectives or not and I've taken few objectives. One you take a sudden turn and go to a street which is full of traffic is a very bad decision. Okay you may reach to the decision faster but that's not a good decision at any cost. Now that's an example and one more thing which you require in an agent is learning, the ability to learn. Usually the agent starts with minimum amount of knowledge and over a period of time it learns okay. From the episode it’s called episodes from episodes agent tends to learn further and further which is quite similar to what we have discussed earlier. There are three things in fact. It learns from how the information that the designer has provided. Second it infers from that input whatever and the third it learns from its own experience. So, these things are important. 

With that we’ll come to an… we come to an end of this particular module. In this particular module we have looked at three things, two algorithms. One is IDA star the other one is RBF both of them are memory optimized algorithms and this particular the third one, third part of this module is about agents. We looked at software and hardware agents which work on… on behalf of users and we have also seen rationality in making a decision is very, very important thing for agents. With that we conclude. Thank you. 
