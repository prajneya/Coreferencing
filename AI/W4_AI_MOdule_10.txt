Welcome friends to the tenth module of AI. We’ll be discussing about the back-propagation algorithm that we mentioned in the previous module. The back-propagation algorithm is one of the most commonly used algorithm and that's why we are going to stress about how that functions and how one can use that algorithm for solving problems which otherwise not possible to be used, not possible to be solved by other means. There are some problems. For example, finding out right set of, now this quite interesting, right set of motivational strategies for employees. Suppose if you have a company where there are thousands of employees working and you would like to retain them, and this is a serious problem in IT actually. So what you do is as an HR manager, you would like to have to strategies designed and defined to make sure that they have more tendency to retain in the organization.

For example, after three years of service you probably give one foreign trip with spouse to every employee or you have if you have two children, you pay the school fees for those two childrens and the other cost you bear those costs and so on so forth. These are called motivational strategies. Now what… what set  of motivational strategies would… will be able to actually retain the employees. Now you like to learn about them. So you may write a program to do it. In fact, one of my PhD student did exactly that. So this is an example where you can use back propagation neural network. The other one, which one of my student did in his project is he was working for a place where a lot of resumes flowing in daily. It’s a huge company and a lot of resumes used to come and they have to have huge bunch of experts to look at those resumes and… and shortlist resumes for further processing and that was very time consuming. So what he did he wrote a neural network based program. So every day there are five thousand applications coming in. It will shortlist five hundred out of that. So experts will have to look at only five hundred resumes and out of which they may figure out that this fifty are good or hundred are good or hundred and twenty are good. So that neural network program could do it. So this is the other example which neural network program can solve. Fingerprint recognition probably is…is something that you have seen quite a lot in recent days. You just place your thumb on the device and it recognizes you as the person or doesn't. So, that is again an example. This is again an example where back propagation algorithm is used. 

Voice recognition systems and face detection systems and there are many, many similar systems used in practice which in sense using back propagation algorithm.  For back propagation algorithm what you need is a multi-layer neuron. Okay multi layer neural network which anyway, I discussed about that in the previous module and we are going to study that thing really more detail in this particular module. Multi layer module means that network contains more than one module, more than one layer, okay. And usually, most of the neural networks use three layers but the recent trend is to use many, many layers, hundreds of layers. The algorithm works like this. So there is a layer one, layer two, layer three. The inputs come from this side and flows through this layers and then goes out the last layer. So the inputs flow in the forward direction and whenever the input flows, the output layer checks whether the output is right or wrong. And if the output is wrong, it floats those errors back and that's why because of this, the algorithm is known as back propagation. It propagates the errors back. When the errors are propagated back, the weights are adjusted accordingly which we discussed in a previous module. 

Now, we are going to study in little more detail. So, this process, the algorithm which does all this, the algorithm tells how you calculate the hind layer, that’s called activation, how what… what flows from the middle layer to the final layer and so on so forth. So that algorithm decides. It also calculates the error coming back and so on so forth. So the algorithm the back-probation algorithm is all about processing like this. But there are some prerequisites to back propagation. The first prerequisite is it works… In fact back propagation is used in many forms in many places. But the one which I'm talking about backpropagation neural network the BPNN which is the most used algorithm requires the network to be complete. What does that mean? The first layer connects to every unit of the second layer, the unit or neuron or number of neurons. So, for example, the input layer we have five neurons - one two three four five. Middle layer you also have say three neurons one two three. Now, the first neuron here connects to first second and third. Second also connects to first second and third and so on. That means it has complete connections. Every input unit or neuron is connected to every middle unit, okay. The same way every middle unit is connected to every output unit and if it has more than one layer, the same thing is applied. So that is called complete network. So that's the first requirement. There is a figure nine point one in the handout as well as in the PPT which clearly visualizes what I just talked about and this is called three three two network. It can be any. So here we have three inputs, three hidden or middle layer units and two output. How number of inputs and output unit and the hidden unit are decided we’ll soon see but before which let us try to understand this thing, this multi layer, feed forward. This is called feed forward. Now that's the other word which is being used feed forward means the inputs are flowing in the forward direction. Remember it applies back propagation algorithm for learning where the errors are propagated back. So the point is inputs are flowing in the forward direction. The errors are flowing in the backward direction. Okay, so that is the essence that’s why it is called feed forward network. The information is coming in the forward direction, the figure clearly indicates how it is moving because it’s moving in the direction of the arrow, okay. 

And you can also see that this is a complete network, every input unit is connected to every middle unit, and every middle unit or hidden layer unit is connected to every output unit. Now how it learns in fact? There are three examples shown on the screen. It’s a nine into nine matrix. And I'm…I’m talking about a very simple case. Nine into nine matrix and I want to recognize A. Okay, so that is a very typical problem that I've taken. This could have been a five hundred into five hundred or five thousand into five thousand pixel image where there is a signature. Thus the discussion becomes a little more complex but the same, okay. The process is the same. In this case we’ll have only eighty one, nine into nine is eighty one inputs. In that case it is five hundred into five hundred inputs and so on so forth. So the network becomes little big pretty complex but the process is going to be the same. You can see that this three A's which are shown here are quite different to each other they are not same. So these are the A's which are provided to the system and you can see that in this case some of the… it’s pretty crude representation but it's still possible for us to understand how it works. You can see that it is there are some pixels which are on there are pixels which are not on, okay. 

So the every image contains some pixels which are black some pixels which are white, okay. Based on this the inputs are provided to the back propagation algorithm and there is an example shown in the next slide. Look at the second figure and look at the data which is generated from that figure. You can see that everywhere there is a white cell we have zero, everywhere there is a black cell we have one. Now this is how it is generated. Now this information is to be provided to the back propagation neural network. You can see that there are total eighty-one inputs out of it some of them are zeros and some of them are ones, okay. So, the next slide talks about this process in a little more detail. Begins with eighty-one has nine into nine total inputs. Now every input… every input of A will be nine into nine, total eighty one we’ll use okay. So that is X one to X eighty-one information provided okay. So every A contain eighty one inputs. Obviously I want to identify not only A but if suppose if I'm interested in finding out all uppercase letters. So it is A to Z that means twenty six, okay. So I have twenty six in that case I need outputs total twenty six outputs. If I also include zeros and ones I’ll have…I need thirty six outputs if I also want lower and upper case it is sixty two total. So what I need, either twenty six, thirty six or sixty two. How many output units do I need? In fact, there are many ways of doing it. One very simple method is to use normal way of using binary neuron as a single digit. If you do that for thirty… less than anything less than thirty two, five units are good. So two raised to five you can have total thirty two combinations which can… which is good for… for example, all zeros will be one. All zeros but the last one is B and all zero but the fourth one is C and so on so forth. So, you have those values differentiated that way. One more you can even add one more. So if you have six units you can have total sixty four possible output.

So based on this we’ll decide number of output units. In this case eighty one input units because it is a nine into nine matrix. The output units will be five or six. How will you decide the hidden layer? The hidden layer number of units people have worked on and decided that the best value for hidden unit, number of hidden numbers can be calculated by using geometric mean of input number of input units and number of hidden the output units. That means multiply both of them and take a square root of it. In this case, you can see that on the slide that we assumed output units to be six,  the input units eighty one. So you multiply them and take a square root of it comes near hundred and… four eighty six. So you take a square root of it is nearly twenty two. So number of hidden units you need are twenty two. So in fact, you may see that why the hidden unit is provided? 

Let me tell you that this neural networks began when there was no hidden unit. There was only one unit which is input there’s one unit which is output. There is no in fact, there is no input unit there is no hidden unit there… there is nothing which you it is called single layer perceptron. And initially it was considered to be a very good thing unless somebody found… there was a guy called Minsky who found that however good this neural networks are, they are incapable of solving some problems, which are… which he called non linearly separable problems. Those problems include a very simple problem called [Indiscernible: 11:19]. If you use only that single layer of perceptron it cannot or perceptron or neural network it cannot. Minsky and this researchers used to call it perceptron. They say that it is impossible for such a network to even differentiate between the values which are [Indiscernible: 11:37]. So it cannot even solve that problem. So people started thinking that neural networks cannot solve this problem long, almost twenty years after long period somebody could find multi-layer, perceptron multi-layer neural network and could actually demonstrate that it can solve almost every problem that comes across. In fact, it is possible for a neural network to solve any problem it can learn. Now the question is whether it can learn or not that is one thing, but it also is theoretically it is possible that it can learn. It is also possible that in fact, empirically it is… it has solved many problems. But again theoretically if you want a proof that the problem is given will surely solve that is not possible. It was… possible in the single layer version. But the multi-layer version you don't have that promise. Fortunately for us the empirical studies the, the practical solutions to this problem proved that it is actually possible for almost all cases. So we don't need to bother much about the theory issue. And there is one more thing as I said that here we have one hidden layer. You can have multiple hidden layers also one hidden layer, the idea of having a hidden layer is to provide some means of finding out features of the input. We’ll soon see but, for example, capital A contains three different features one slanting line like this, the other slanting line like this. One more is a horizontal line whenever these three features exist, we assume that it is A, irrespective of the size or orientation or wherever it is. 

Okay so if you can do that, what is the advantage? The advantage is that we'll be able to generalize any size any… any orientation anywhere, if you got two different slanting lines in different direction and a horizontal line it is A. So that can generalize. So the beauty of having a hidden layer is that it can figure out the features from the input and based on those features, the output unit can learn to become active or inactive. It is possible to have multiple hidden layers and again the theory says that it is possible to…to use a single hidden layer and a single hidden layer can do a job which multiple hidden layers can do. So it is it is something which says that one single hidden layer is suffice, is good enough for solving any problem. But then later on researchers found that if you want to get a…if you have a very, very complex figure, where a feature itself is made up of other features, what you need is multiple layers. And today as I said, there is something called Deep Learning which is happening where you have lot of layers and there is a modified algorithm. Not BPNN is not used a little modified algorithm is used there. And as I said, the algorithms are not foolproof. The algorithm may not correctly classify. In that case, in most cases you have to rerun the program. You may be surprised rerunning  the program how it solves? Because, as I said, you begin with random set of weights. If you… if you look mathematically a random set of N weight is a point in an N dimensional space. Obviously, we want to get the correct values of those N inputs which can identify every input correctly. 

So there is another point. So what you need to do is to travel in N dimensional space from the randoms point that you start with to the goal state. The issue is, that if you choose… run the program again it’ll pick up some other random point and start traveling towards goal state again. Now if there is a local minima or local maxima whatever, when it is moving around and it falls into local minima or climbs up local maxima, it cannot go forward. But if you run it again it starts from some other point, other place on the same plain it will probably be able to reach. 

Okay. So that's the reason why what you need to do is to rerun the program. In most cases that will solve your problem. The next slide talks about how hidden unit learns to be active. For example, I have three inputs A, I and H and I am showing three different images of A but two images of… one each of I and H. Okay so there is a unit one which learns to be active when the line… one slanting line from going up from left to right okay. These are present in the image. One more line, one more hidden unit learn to become active and it is… it is right it’s… it’s… it’s going down from right to…. left to right. Third unit will learn to become active and the horizontal line [Indiscernible: 16:21]. You can see that the second slide that the hidden unit which learn to active and there is an horizontal line also learns to be active and there is I and there is H. 

Okay. So it is possible that the different hidden units learn to remain active when there is a typical feature present in the file the present in the image. How output unit learns to remain active and for example is provided to it. It is again pretty simple. Just look at the hidden unit which it, just look at 3 hidden units one slanting line one with the… this one's slanting and the other one is on the other slanting and the third one is the horizontal left to right up, left to right down and horizontal. When these three lines remain active, these three hidden units become active, the unit which wants to learn to become active and A is presented will learn to become active. So okay, so that's how it learns. You can see that the third unit learns to become active and there is the other line. So the figures, the three figures in the handout as well as the PPT clearly indicates this part. 

Let me recap this… this is part is pretty important. We have three units. One learns to become active and there is a slanting line moving from left to right up. There’s the other slanting line which moves from left to right down and the other one which is horizontal. Every unit learns to pick up this feature and learn to become active. So, when there are many other cases. For example, in case of G, okay, you also have here you have a horizontal line. The issue is that in case of G if in most handwritten case recognition, you will not find that horizontal line. But in case of A, you will almost invariably find them. So these are also points that one must learn but it is very interesting how back propagation manages it. 


What you're supposed to do is to provide some images of G with and some images without that horizontal line. So, it learns to… to accept G even without that. So it… it’s very important that you provide right set of samples, right set of images. And that's why we have to provide number of images, okay. So more numbers you provide, in most cases it is better. But then there is a limit also, you can't, you should not be going beyond that limit, and we'll be discussing about that soon. Now, let us look at this back propagation network. Now this back propagation network is depicted here with the…the dark boldface lines and the weights. You can see that there are, now this is only an extension to what we have seen in the earlier module we have X0 to XN as an input. Now the N value depends on the features that you take from the input we have already seen that. In our character recognition case the N value is eighty one okay. Then you have the middle layer in… remember middle layer you have to total N units. And remember in our previous example that we took M value is twenty-two. Okay and you have output units. In our case, it is here. I've taken L. So L number of output units. What is the value of N in the previous example? Twenty-six. If you only consider uppercase, thirty-six uppercase plus the digits and fifty-two when you consider upper lowercase and together and sixty-two if it is upper lower case as well as digits.

Okay so we are considering L to be six, then all three cases are… are possible to be provided, Okay. So you can use them. Now in… in look at the weights okay. Now those weights every remember every input unit is connected to every output unit. So there is a weight between X zero and H one. There is weight between X one, okay, remember the X zero and H zero look like inputs but you can see that there is no arrow going inside. They are not inputs. X zero and H zero are one. Okay remember W zero indicates threshold, the negative value of threshold. Similarly at H zero also indicate… the W zero in that case H zero, the second layer, okay. So that also it is the negative value of threshold. 

Now there are two different set of weights, okay. There are in other words there are three layers of inputs but two layers of weights. So, the first layer of weight is called W one the second layer of weight is called W two. The first layer and you can say W one prefixes always, okay. The… the value, the which comes next the suffix, zero one that value indicates which input unit to which output unit. For example, W one zero one means a weight between zeroth input unit to first hidden unit. W two zero one means the weight between the zeroth or the first time hidden unit to the output unit one, remember. That… that… that is what the indication is and that's how it is done. Now how it is summed up? Remember the calculation we requires everything every the sigma that we have seen X X one W one  X two W two X three W three and so on. And when you consider the threshold you also have X zero W zero where X zero is basically one. So the summation, suppose if I want to calculate the sum that is happening at H… H one how it is calculated is shown in the next slide. We can say that H one equal to why is one because X zero is always one into W one zero one. Then W plus now you talk about the next input. X one into W one one one. X two into W one two one X three W three one.

Okay so on and till the last one and remember total values are M okay. So we have the last one is W one M one okay. Similarly at Ith hidden unit it shows on the screen the only difference is that it was one, the second suffix was one which becomes I now. Okay so that's the only difference otherwise it's the same okay. So that is basically summation and how HI values are calculated we have already seen that. We’ll use sigmoid function sigmoid function is used here. So that is one upon one plus E raised to minus summation and the summation we've already seen okay. It shows that J equal to  zero to N. Okay the hidden unit total N inputs are provided. N inputs are presented. So it is 0 to N okay and you… what you add? You add  XI WI  ok so those XJ and WJ are multiplied okay. So that is what is being done here.

Same way the output unit all inputs, all hidden units connected to it they are… they are all multiplied HY WY HI WIJ In fact IJ if is the Jth unit HI JI H one J. Assume this is J. H one H one W one J H one W two J and so on. So, it… it calculates it that way. So, total summation is calculated same way one upon one plus E raised to two minus some is calculated which you can see on the screen. So we call them equation ten point one and ten point two which are, which indicates the output generated for both of them. 

With that we’ll come to an end of this particular module. We have looked at back propagation algorithm and this is all the… this is prerequisite to back propagation algorithm. We’ll see in the next module how exactly back propagation is going to solve the problem but this, even the algorithm we’ll look at the complete algorithm the next module. In this module we have seen that how this thing is divided into layers, how the activations flow, how the values are calculated, how the weights are named. W one is layer one W two is layer two I and J. So I the input unit and J hidden unit W one I J W two I J is Ith  hidden unit to Jth  output unit. Okay we have also seen how you decide number of input output and hidden units. Number of input units is decided based on the input okay, the size of file that you take for input. Number of output units depends on number of outputs that you would like to classify. Number of hidden units is taken as a geometric mean of input and output. 

Okay, so, and we have taken a character recognition example. The job of a hidden layer is… is to identify features okay. And that that’ll help in generalization because if you identify features irrespective of the input the pixel value is it becomes possible, it makes it possible to generalize. For example, here you get a slanting line like this and this and a horizontal line you can call it A irrespective of which picture, which pixel it illuminates and which pixel it doesn't. Okay, so with that note, like to conclude this module. Thank you all. 
